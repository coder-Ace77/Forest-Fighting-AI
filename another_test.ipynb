{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import math\n",
    "import random\n",
    "from collections import Counter\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Forest:\n",
    "    def __init__(\n",
    "        self,\n",
    "        size,\n",
    "        indiviual_reward_importance,\n",
    "        social_reward_importance,\n",
    "        p_change_wind,\n",
    "        type_plane,\n",
    "        P_burn,\n",
    "        P_set_fire,\n",
    "        P_fire_depend_on_wind,\n",
    "    ):\n",
    "        self.indiviual_reward_importance = indiviual_reward_importance  # int\n",
    "        self.social_reward_importance = social_reward_importance  # int\n",
    "        self.grid_env = np.zeros((size, size))  # 0: safe, 1: on fire, 2:burnt\n",
    "        self.grid_type = np.zeros(\n",
    "            (size, size)\n",
    "        )  # -1: station, 0: non_tree, 1: tree, 2: home\n",
    "        self.init_grid(type_plane)\n",
    "        self.wind_strength = 1\n",
    "        self.wind_direction = 0  # four direction\n",
    "        self.p_change_wind = p_change_wind  # 0 or 0.0001\n",
    "        self.change_wind()\n",
    "        self.P_burn = P_burn  # on fire=>burntd\n",
    "        self.P_set_fire = P_set_fire  # neighbourhood on fire\n",
    "        self.P_fire_depend_on_wind = (\n",
    "            P_fire_depend_on_wind  # neighbour on fire != direction wind => +, else =>-\n",
    "        )\n",
    "\n",
    "        self.episode = []\n",
    "        self.burnt_trees = 0\n",
    "        self.fired_trees = 0\n",
    "        self.init_fire(1)\n",
    "\n",
    "    #  ! This method generated plateau it takes type plane which specifies the type of plane\n",
    "    # ! and the probability of each type of plane [[0,1,2],[p(0),p(1),p(2)]]\n",
    "\n",
    "    def init_grid(self, type_plane):\n",
    "        # * -1: station, 0: non_tree, 1: tree, 2: home\n",
    "\n",
    "        m = len(self.grid_type)\n",
    "\n",
    "        station_position = [\n",
    "            np.random.choice(range(m)),\n",
    "            np.random.choice(range(m)),\n",
    "        ]\n",
    "\n",
    "        # * station position has been decided at random for each episode\n",
    "\n",
    "        for i in range(m):\n",
    "            for j in range(m):\n",
    "                self.grid_type[i][j] = np.random.choice(type_plane[0], p=type_plane[1])\n",
    "        winSize = 1\n",
    "        for i in range(m - 2 * winSize):\n",
    "            for j in range(m - 2 * winSize):\n",
    "                self.grid_type[i + winSize][j + winSize] = int(\n",
    "                    np.mean(self.grid_type[i : i + 2 * winSize, j : j + 2 * winSize])\n",
    "                )\n",
    "        self.grid_type[station_position[0]][station_position[1]] = -1\n",
    "        # print(self.grid_type)\n",
    "        return self.grid_type\n",
    "\n",
    "    # ! this function changes wind direction as well as speed we will change wind direction after each episode, wind direction can deviate 20% in each direction\n",
    "\n",
    "    def change_wind(self):\n",
    "        # *Output : self.wind_strength, self.wind_direction\n",
    "        # *Choose the wind direction and its strenghth randomly.\n",
    "        # *0 : Right , 1 : Up , 2 : left , 3 : Down\n",
    "\n",
    "        self.wind_direction = random.choice([0, 1, 2, 3])\n",
    "        self.wind_strength += random.uniform(-0.2, 0.2) * self.wind_strength\n",
    "        self.wind_strength = float(\"{:.3f}\".format(self.wind_strength))\n",
    "        return self.wind_strength, self.wind_direction\n",
    "\n",
    "    # TODO to optimise this function\n",
    "    # ! Just returns the neighbours of given node\n",
    "\n",
    "    def neighbours(self, i, j):\n",
    "        # * it reresents each neighbour as dictionary where 0 ,1 , 2 ,3 reperesents directions\n",
    "        # *0 : Right , 1 : Up , 2 : left , 3 : Down (i is columns and j is row)\n",
    "\n",
    "        neighbours = {\n",
    "            \"0\": (i, j + 1),\n",
    "            \"1\": (i - 1, j),\n",
    "            \"2\": (i, j - 1),\n",
    "            \"3\": (i + 1, j),\n",
    "        }\n",
    "        if i == 0 and j == 0:\n",
    "            neighbours = {\"0\": (i, j + 1), \"3\": (i + 1, j)}\n",
    "        elif i == 0 and j == self.grid_env.shape[0] - 1:\n",
    "            neighbours = {\"2\": (i, j - 1), \"3\": (i + 1, j)}\n",
    "        elif i == self.grid_env.shape[0] - 1 and j == 0:\n",
    "            neighbours = {\"0\": (i, j + 1), \"1\": (i - 1, j)}\n",
    "        elif i == self.grid_env.shape[0] - 1 and j == self.grid_env.shape[0] - 1:\n",
    "            neighbours = {\"1\": (i - 1, j), \"2\": (i, j - 1)}\n",
    "        elif i == 0:\n",
    "            neighbours = {\"0\": (i, j + 1), \"2\": (i, j - 1), \"3\": (i + 1, j)}\n",
    "        elif i == self.grid_env.shape[0] - 1:\n",
    "            neighbours = {\"0\": (i, j + 1), \"1\": (i - 1, j), \"2\": (i, j - 1)}\n",
    "        elif j == 0:\n",
    "            neighbours = {\"0\": (i, j + 1), \"1\": (i - 1, j), \"3\": (i + 1, j)}\n",
    "        elif j == self.grid_env.shape[0] - 1:\n",
    "            neighbours = {\"1\": (i - 1, j), \"2\": (i, j - 1), \"3\": (i + 1, j)}\n",
    "        return neighbours\n",
    "\n",
    "    # ! A function to start fire init point number is just the no of points where fire will be lightned\n",
    "\n",
    "    def init_fire(self, init_point_number):  # return mean position\n",
    "        # * random between 1 to 4\n",
    "        # * check not station and none_tree\n",
    "\n",
    "        points = []\n",
    "        size = self.grid_env.shape[0]\n",
    "        for i in range(init_point_number):\n",
    "            flag = True\n",
    "            while flag:\n",
    "                X1 = np.random.choice(range(size))\n",
    "                X2 = np.random.choice(range(size))\n",
    "                if self.grid_type[X1][X2] >= 1:\n",
    "                    points.append((X1, X2))\n",
    "                    break\n",
    "            self.grid_env[X1][X2] = 1\n",
    "            neighbours = list(self.neighbours(X1, X2).values())\n",
    "            for i in neighbours:\n",
    "                self.grid_env[i[0]][i[1]] = 1\n",
    "        return points\n",
    "\n",
    "    # ! returns neighbours in fire and their direction It just returns the neightbours which are on fire but\n",
    "    # ! not returns the absolute coordinate but returns the direction ie either [0,2] ie which neighbours\n",
    "\n",
    "    def get_neighbours_on_fire(self, grid_env, point):\n",
    "        on_fires_direction = []\n",
    "        # * 0 : Right || 1 :Up || 2 : Left || 3 : Down\n",
    "        i = point[0]\n",
    "        j = point[1]\n",
    "        neighbours = self.neighbours(i, j)\n",
    "        for n in range(len(neighbours)):\n",
    "            neighbour = list(neighbours.values())[n]\n",
    "            direction = list(neighbours.keys())[n]\n",
    "            if grid_env[neighbour[0]][neighbour[1]]:\n",
    "                on_fires_direction.append(int(direction))\n",
    "        return on_fires_direction\n",
    "\n",
    "    # ! Reset the game for next episode\n",
    "\n",
    "    def reset(self, type_plane):  # init\n",
    "        size = self.grid_env.shape[0]\n",
    "        self.wind_strength = 0\n",
    "        self.wind_direction = 0\n",
    "        self.episode = []\n",
    "        self.grid_env = np.zeros((size, size))\n",
    "        self.grid_type = np.zeros((size, size))\n",
    "        self.init_grid(type_plane)\n",
    "        self.change_wind()\n",
    "        self.init_fire(1)\n",
    "\n",
    "    # ? Simulation functions\n",
    "\n",
    "    # TODO Wrong code\n",
    "    # ! It will take list of neighboursa and adjust probablity based on fire direction ie on fire direction\n",
    "    # ! It is more likely to spread the fire\n",
    "    def get_p_fire(self, alpha, neighbours_on_fire):\n",
    "        \"\"\"\n",
    "        input : gets neighbours on fire and their direction\n",
    "        output: calculate p_fire base on wind direction\n",
    "        \"\"\"\n",
    "        p_fire = 1\n",
    "        higher_alpha = 0.9\n",
    "        for i in neighbours_on_fire:\n",
    "            if i == self.wind_direction:\n",
    "                p_fire = p_fire * higher_alpha\n",
    "            else:\n",
    "                p_fire = p_fire * alpha\n",
    "        return p_fire\n",
    "\n",
    "    # ! Main simulation function which changes the states\n",
    "    # * burnt states are not transitioned\n",
    "    # * If a given node has its neighbours on fire then this node will have a prob to be set on fire\n",
    "    # TODO First of all descrese time complexity of grid updating , second need to upgrade the model\n",
    "    # To discuss new model of fire updation\n",
    "\n",
    "    # * Working for every node we see which neighbours are on fire and then update current node according to prob\n",
    "\n",
    "    def P_transitions(self):\n",
    "        alpha = 0.7  # * prob to be set on fire opposite to wind direction\n",
    "        size = self.grid_env.shape\n",
    "        copy_env = np.zeros(size) + self.grid_env\n",
    "        for i in range(size[0]):\n",
    "            for j in range(size[1]):\n",
    "                on_fire_neighbours = self.get_neighbours_on_fire(copy_env, (i, j))\n",
    "                if copy_env[i][j] == 0:\n",
    "                    if self.grid_type[i][j] != -1 and self.grid_type[i][j] != 0:\n",
    "                        p_fire = self.get_p_fire(alpha, on_fire_neighbours)\n",
    "                        # alpha based on wind direction\n",
    "                        # 0 : Right , 1 : Up , 2 : left , 3 : Down\n",
    "                        new_state = np.random.choice([0, 1], p=[p_fire, 1 - p_fire])\n",
    "                        self.grid_env[i][j] = new_state\n",
    "                elif copy_env[i][j] == 1:\n",
    "                    newstate = np.random.choice(\n",
    "                        [1, 2], p=[1 - self.P_burn, self.P_burn]\n",
    "                    )\n",
    "                    self.grid_env[i][j] = newstate\n",
    "                    if newstate == 2:\n",
    "                        neighbours = list(self.neighbours(i, j).values())\n",
    "                        for n in neighbours:\n",
    "                            if self.grid_env[n] == 0 and self.grid_type[i][j] > 0:\n",
    "                                self.grid_env[n] = 1\n",
    "                                break\n",
    "\n",
    "    # ! returns no of trees burnt between and no of trees caught fire between two states\n",
    "    # TODO my inference is that this is calculating burnt trees in wrong way\n",
    "\n",
    "    def get_env_give_tree(self, first_env, second_env):\n",
    "        safe_counter, new_safe_counter = 0, 0\n",
    "        burnt_counter, new_burnt_counter = 0, 0\n",
    "        for i in range(first_env.shape[0]):\n",
    "            for j in range(first_env.shape[1]):\n",
    "                if first_env[i][j] == 0:\n",
    "                    safe_counter += 1\n",
    "                elif first_env[i][j] == 2:\n",
    "                    burnt_counter += 1\n",
    "        for i in range(first_env.shape[0]):\n",
    "            for j in range(first_env.shape[1]):\n",
    "                if second_env[i][j] == 0:\n",
    "                    new_safe_counter += 1\n",
    "                elif second_env[i][j] == 2:\n",
    "                    new_burnt_counter += 1\n",
    "        # * Safe counter is the number of healthy trees in a state\n",
    "        # * burnt_counter is no of trees completely burnt in a state\n",
    "        burnt_trees = new_burnt_counter - burnt_counter\n",
    "        fired_trees = safe_counter - new_safe_counter\n",
    "        return burnt_trees, fired_trees\n",
    "\n",
    "    # ! returns weather current cell is at border or not\n",
    "    # TODO wrong implementation\n",
    "    def is_border(self, x, y):\n",
    "        # * This function get the position of the agent and return True if there is one safe tree or home grid with manhatan distance = 1\n",
    "        is_border = False\n",
    "        neighbors = list(self.neighbours(x, y).values())\n",
    "        # 0: safe, 1: on fire, 2:burnt\n",
    "        if self.grid_env[x, y] != 1:\n",
    "            return is_border\n",
    "        for neigh in neighbors:\n",
    "            if self.grid_env[neigh] == 0 and self.grid_type[neigh] > 0:\n",
    "                is_border = True\n",
    "                break\n",
    "        return is_border\n",
    "\n",
    "    # ! This function takes action and postion of agent as input and computes next state\n",
    "    def next_state(self, action, position, env_transition=True):\n",
    "        \"\"\"\n",
    "        Input : action : include control actions of all agents || env_transition = determine whether to change environment based on passing time\n",
    "        Output: Updated self.grid_env\n",
    "        Change the dynamic of environment based on agents' actions or passing time.\n",
    "        \"\"\"\n",
    "        grid_env_p = np.copy(self.grid_env)\n",
    "        if action is not None:\n",
    "            for i in range(len(action)):\n",
    "                x, y = position[i]\n",
    "                if action[i][0] == 1 and self.grid_env[x, y] == 1:\n",
    "                    self.grid_env[x, y] = 0\n",
    "\n",
    "        if env_transition:\n",
    "            # call P_transition method\n",
    "            self.P_transitions()\n",
    "            # for idx, x in np.ndenumerate(self.grid_env):\n",
    "            #     if x ==  0:\n",
    "            #         self.grid_env[idx] = random.choices([0,1,2],weights = self.P_transition[0,:])[0]\n",
    "            #     elif x == 1:\n",
    "            #         self.grid_env[idx] = random.choices([0,1,2],weights = self.P_transition[1,:])[0]\n",
    "        # self.burnt_trees, self.fired_trees = self.get_env_give_tree(grid_env_p, self.grid_env)\n",
    "        return self.grid_env\n",
    "\n",
    "    # ! Reward calculation function for all agents based on their actions and positions\n",
    "    # TODO wrong implementation (why?) environment is considered deterministic\n",
    "\n",
    "    def calculate_reward(self, action, position):\n",
    "        # state=[3X3],[3X3],x,y,mean_x_fire,mean_y_fire\n",
    "        # Action: fire_r => 0,1 , move => 0,...,8, help => 0,1, =>[[0,1],[0,...,8],[0,1]]\n",
    "        # individual: collision => -- , healthy retardant => -, on_fire on border=> ++, on_fire=>+,\n",
    "        # social: for each new burnt tree=>-, for each new burnt home=>--,\n",
    "        # action = fire_retardant:\n",
    "        # ind_r = if home: on_fire => safe => 3*r1\n",
    "        #         elif tree: on_fire => safe => 2*r1\n",
    "        #         elif   station or non_tree => -2*r1\n",
    "        #         border(at least one neigh safe and not non_tree): on_fire => safe => r1\n",
    "        # move => if collision: -10*r1\n",
    "        # help => -10*r1\n",
    "\n",
    "        r1 = 5\n",
    "        new_position = position\n",
    "        ind_r = np.zeros((len(action), 1))\n",
    "        social_r = np.zeros((len(action), 1))\n",
    "        num_rep_pos = dict(Counter(new_position))\n",
    "        for i in range(len(action)):\n",
    "            x, y = position[i]\n",
    "            if action[i][0] == 1:\n",
    "                if self.grid_env[x, y] == 0 or self.grid_env[x, y] == 2:\n",
    "                    ind_r[i, :] -= r1\n",
    "                    if self.grid_type[x, y] == -1 or self.grid_type[x, y] == 0:\n",
    "                        ind_r[i, :] -= r1\n",
    "\n",
    "                else:\n",
    "                    if self.grid_type[x, y] == 1:\n",
    "                        ind_r[i, :] += 2 * r1\n",
    "                    elif self.grid_type[x, y] == 2:\n",
    "                        ind_r[i, :] += 3 * r1\n",
    "                    if self.is_border(x, y) == True:\n",
    "                        ind_r[i, :] += r1\n",
    "                ind_r[i, :] += 10 * r1\n",
    "            if num_rep_pos[new_position[i]] > 1:\n",
    "                ind_r[i, :] -= 10 * r1\n",
    "            if action[i][2] == 1:\n",
    "                ind_r[i, :] -= 10 * r1\n",
    "            social_r[i, :] = -1 * r1 * self.burnt_trees - 1 * r1 * self.fired_trees\n",
    "\n",
    "        # social_r:\n",
    "        # -1*#new_burnt -1*#new_home_burnt -1*#new_on_fire -1*#new_home_on_fire\n",
    "        # returns self.indiviual_reward_importance* ind_r + self.social_reward_importance * social_r for each agent\n",
    "\n",
    "        total_r = (\n",
    "            self.indiviual_reward_importance * ind_r\n",
    "            + self.social_reward_importance * social_r\n",
    "        )\n",
    "\n",
    "        # print(\"ind reward : \", ind_r)\n",
    "        # print(\"social reward : \", social_r)\n",
    "        return total_r\n",
    "\n",
    "    # ! returns flase if any one on fire otherwise true\n",
    "    # TODO can  be integrated into p transitons\n",
    "    def terminated(self):  # no on fire\n",
    "        terminated = True\n",
    "        for i in range(self.grid_env.shape[0]):\n",
    "            for j in range(self.grid_env.shape[0]):\n",
    "                if self.grid_env[i][j] == 1:\n",
    "                    terminated = False\n",
    "                    break\n",
    "        return terminated\n",
    "\n",
    "    # # ! returns the view of single agent it is lXw grid around agent\n",
    "    # def observe(self, l, w, position):\n",
    "    #     \"\"\"\n",
    "    #     Input : l: length of scope of IR Camera || w : length of scope of IR Camera || postion : one Agent position\n",
    "    #     Output : Situatuion of l*w grids in the agent scope\n",
    "    #     A downward facing Infrared (IR) camera captures the states of an l * w sized grid of trees.The agent is located at the center         of the image and if images are taken at an edge of the forest, the image is padded with non-trees.\n",
    "    #     \"\"\"\n",
    "    #     assert l % 2 == 0 and w % 2 == 0, \"l and w should be an even integer\"\n",
    "    #     x, y = position\n",
    "    #     X, Y = self.grid_env.shape\n",
    "    #     X = X - 1\n",
    "    #     Y = Y - 1\n",
    "    #     dpad_x = max(int(x + l / 2) - X, 0)\n",
    "    #     upad_x = max(0 - int(x - l / 2), 0)\n",
    "    #     rpad_y = max(int(y + w / 2) - Y, 0)\n",
    "    #     lpad_y = max(0 - int(y - w / 2), 0)\n",
    "    #     # print(lpad_y,rpad_y,upad_x,dpad_x)\n",
    "    #     grid1 = np.pad(\n",
    "    #         self.grid_env,\n",
    "    #         ((upad_x, dpad_x), (lpad_y, rpad_y)),\n",
    "    #         \"constant\",\n",
    "    #         constant_values=0,\n",
    "    #     )\n",
    "    #     grid2 = np.pad(\n",
    "    #         self.grid_type,\n",
    "    #         ((upad_x, dpad_x), (lpad_y, rpad_y)),\n",
    "    #         \"constant\",\n",
    "    #         constant_values=0,\n",
    "    #     )\n",
    "    #     print(int(x- l/2)+upad_x,int(x + l/2)+upad_x+1,int(y-w/2)+lpad_y, int(y+w/2)+lpad_y+1)\n",
    "    #     print(grid1.shape)\n",
    "    #     return (\n",
    "    #         grid1[\n",
    "    #             int(x - l / 2) + upad_x : int(x + l / 2) + upad_x + 1,\n",
    "    #             int(y - w / 2) + lpad_y : int(y + w / 2) + lpad_y + 1,\n",
    "    #         ],\n",
    "    #         grid2[\n",
    "    #             int(x - l / 2) + upad_x : int(x + l / 2) + upad_x + 1,\n",
    "    #             int(y - w / 2) + lpad_y : int(y + w / 2) + lpad_y + 1,\n",
    "    #         ],\n",
    "    #     )\n",
    "\n",
    "    # def observe(self,l,w,position):\n",
    "    #     f_r = 0\n",
    "    #     f_c = 0\n",
    "\n",
    "    #     n , m = self.grid_env.shape\n",
    "    #     x , y = position\n",
    "\n",
    "    #     mn = min()\n",
    "\n",
    "    #     grid1 = np.zeros((f_r,f_c))\n",
    "    #     return grid1\n",
    "    \n",
    "    # ! takes ations and positions of agents and returns reward , episode over and observation for each agent\n",
    "    def step(self, action, position):\n",
    "        self.next_state(action, position)\n",
    "        r = self.calculate_reward(action, position)\n",
    "        obsv = []\n",
    "        for p in position:\n",
    "            obsv.append(self.observe(2, 2, p))\n",
    "        return r, self.terminated(), obsv\n",
    "    \n",
    "\n",
    "    # ! returns mean position of fire\n",
    "    def mean_fire_pos(self):\n",
    "        fire_pos = []\n",
    "        for idx in range(len(self.grid_env)):\n",
    "            for idx2 in range(len(self.grid_env)):\n",
    "                if self.grid_env[idx][idx2] == 1:\n",
    "                    fire_pos.append(np.array([idx, idx2]))\n",
    "        if len(np.array(fire_pos)) == 0:\n",
    "            return 0, 0\n",
    "        a = np.average(np.array(fire_pos), axis=0)\n",
    "        return int(a[0]), int(a[1])\n",
    "\n",
    "\n",
    "    # ? utilities for rendering and saving episodes\n",
    "    # TODO implement again using pygame library\n",
    "\n",
    "    def render(self, agents_position):  # opencv, show state, show info\n",
    "        m = len(self.grid_type)\n",
    "        pad = 10\n",
    "        img = np.zeros((m * pad + pad, m * pad, 3), np.uint8)\n",
    "        for i in range(m):\n",
    "            for j in range(m):\n",
    "                if self.grid_env[i][j] == 0:\n",
    "                    if self.grid_type[i][j] == 2:\n",
    "                        c = (190, 0, 0)\n",
    "                    elif self.grid_type[i][j] == -1:\n",
    "                        c = (0, 240, 240)\n",
    "                    elif self.grid_type[i][j] == 1:\n",
    "                        c = (0, 190, 0)\n",
    "                    else:\n",
    "                        c = (30, 105, 210)\n",
    "                elif self.grid_env[i][j] == 1:\n",
    "                    c = (0, 0, 247)\n",
    "                else:\n",
    "                    c = (0, 0, 0)\n",
    "                img[i * pad : (i + 1) * pad, j * pad : (j + 1) * pad] = c\n",
    "        font = cv.FONT_HERSHEY_SIMPLEX\n",
    "        bottomLeftCornerOfText = (0, m * pad + pad)\n",
    "        fontScale = 1\n",
    "        fontColor = (255, 255, 255)\n",
    "        lineType = 2\n",
    "        txt = (\n",
    "            \"Wind Direction:\"\n",
    "            + str(self.wind_direction)\n",
    "            + \" strength:\"\n",
    "            + str(self.wind_strength)\n",
    "        )\n",
    "        cv.putText(\n",
    "            img, txt, bottomLeftCornerOfText, font, fontScale, fontColor, lineType\n",
    "        )\n",
    "        for a in agents_position:\n",
    "            cv.circle(\n",
    "                img,\n",
    "                (a[0] * pad + int(pad / 2), a[1] * pad + int(pad / 2)),\n",
    "                int(pad / 2),\n",
    "                (255, 0, 255),\n",
    "                -1,\n",
    "            )\n",
    "        cv.imwrite(\"T.jpg\", img)\n",
    "        self.episode.append(img)\n",
    "        self.episode.append(img)\n",
    "        self.episode.append(img)\n",
    "\n",
    "    def save_episode(self):  # save episode to video\n",
    "        m = len(self.grid_type)\n",
    "        pad = 10\n",
    "        frameSize = (m * pad + pad, m * pad)\n",
    "        video_path = \"out.avi\"\n",
    "        height, width, channels = self.episode[0].shape\n",
    "        video = cv.VideoWriter(\n",
    "            video_path, cv.VideoWriter_fourcc(*\"DIVX\"), 10, (width, height)\n",
    "        )\n",
    "        for img in self.episode:\n",
    "            video.write(img)\n",
    "\n",
    "        cv.destroyAllWindows()\n",
    "        video.release()\n",
    "        print(\"video saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = Forest(\n",
    "    size=20,\n",
    "    indiviual_reward_importance=5,\n",
    "    social_reward_importance=2,\n",
    "    p_change_wind=0,\n",
    "    type_plane=[[0, 1, 2], [0.005, 0.9, 0.095]],\n",
    "    P_burn=0.1,\n",
    "    P_set_fire=0.1,\n",
    "    P_fire_depend_on_wind=0.1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep neural netwrok for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, lr, input_dims, fc1_dims, fc2_dims, n_actions):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dims,fc1_dims),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(fc1_dims, fc2_dims),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(fc2_dims, n_actions),\n",
    "        )\n",
    "\n",
    "    def forward(self, features):\n",
    "        return self.network(features.float())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mini_Agent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        id,\n",
    "        level,\n",
    "        parent,\n",
    "        env,\n",
    "        read=None,\n",
    "        input_dims=22,\n",
    "        n_actions=36,\n",
    "        batch_size=16,\n",
    "        gamma=0.9,\n",
    "        epsilon=0.9,\n",
    "        lr=0.1,\n",
    "        max_mem_size=10000,\n",
    "        eps_end=0.05,\n",
    "        eps_dec=5e-4,\n",
    "    ):\n",
    "        self.model = DeepQNetwork(\n",
    "            lr, n_actions=n_actions, input_dims=input_dims, fc1_dims=128, fc2_dims=128\n",
    "        )\n",
    "        self.target = DeepQNetwork(\n",
    "            lr, n_actions=n_actions, input_dims=input_dims, fc1_dims=128, fc2_dims=128\n",
    "        )\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "        self.sars = None\n",
    "        self.eps = 1\n",
    "        self.reward_history = []\n",
    "        self.loss_history = []\n",
    "        self.num_train_episodes = 0\n",
    "        self.print_enough_experiences = False\n",
    "\n",
    "        self.id = id\n",
    "        self.level = level\n",
    "        self.n_actions = n_actions\n",
    "        self.parent = parent\n",
    "        # random ?? np.array or tuple ??\n",
    "        size = env.grid_env.shape[0]\n",
    "        x = random.randint(0, size - 1)\n",
    "        y = random.randint(0, size - 1)\n",
    "        self.position = (x, y)\n",
    "        # Broadcast ??\n",
    "        mean_x_fire, mean_y_fire = env.mean_fire_pos()\n",
    "        self.mean_pos_fire = (mean_x_fire, mean_y_fire)\n",
    "\n",
    "        self.observ1, self.observ2 = env.observe(2, 2, self.position)\n",
    "        self.observ = [self.observ1, self.observ2, self.position, self.mean_pos_fire]\n",
    "        self.best_SARS = [self.observ, [0, 0, 0], 0, self.observ, 0]\n",
    "        self.action_list = []\n",
    "        for i in range(2):\n",
    "            for j in range(9):\n",
    "                for k in range(2):\n",
    "                    self.action_list.append([i, j, k])\n",
    "        # id,memory,...\n",
    "        # position\n",
    "        # meanx, meany\n",
    "        # best_state_action_R\n",
    "        self.action_space = range(n_actions)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_min = eps_end\n",
    "        self.eps_dec = eps_dec\n",
    "        self.lr = lr\n",
    "        self.past_position = (0, 0)\n",
    "        self.configdtype = torch.tensor\n",
    "        self.mem_size = max_mem_size\n",
    "        self.batch_size = batch_size\n",
    "        self.replace_target = 4\n",
    "        self.min_exp = 10\n",
    "        self.loss_fn = nn.MSELoss(reduction=\"mean\")\n",
    "        self.input_dims = input_dims\n",
    "        self.model_updates = 0\n",
    "        if read is not None:\n",
    "            self.load_checkpoint(read)\n",
    "        \"\"\"\n",
    "        self.mem_cntr = 0\n",
    "        self.iter_cntr = 0\n",
    "        # n_actions,input_dims in the input ??\n",
    "        self.Q_eval = DeepQNetwork(lr, n_actions=n_actions, input_dims=input_dims,\n",
    "                                    fc1_dims=128, fc2_dims=128)\n",
    "        self.Q_next = DeepQNetwork(lr, n_actions=n_actions, input_dims=input_dims,\n",
    "                                    fc1_dims=64, fc2_dims=64)\n",
    "                            \n",
    "        self.state_memory = np.zeros((self.mem_size, input_dims), dtype=np.float32)\n",
    "        self.new_state_memory = np.zeros((self.mem_size, input_dims), dtype=np.float32)\n",
    "        self.action_memory = np.zeros((self.mem_size,n_actions), dtype=np.int32)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)\"\"\"\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        checkpoint = {\n",
    "            \"state_dict\": self.model.state_dict(),\n",
    "            \"target_dict\": self.target.state_dict(),\n",
    "            \"optimizer\": self.optimizer.state_dict(),\n",
    "            \"replay\": self.sars,\n",
    "            \"epsilon\": self.eps,\n",
    "            \"reward_history\": self.reward_history,\n",
    "            \"loss_history\": self.loss_history,\n",
    "            \"num_train_episodes\": self.num_train_episodes,\n",
    "        }\n",
    "        filename = \"madqn-\" + str(self.id) + \".pth.tar\"\n",
    "        torch.save(checkpoint, filename)\n",
    "\n",
    "    def load_checkpoint(self, id):\n",
    "        filename = \"madqn-\" + str(id) + \".pth.tar\"\n",
    "        checkpoint = torch.load(filename)\n",
    "        self.model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "\n",
    "        self.target.load_state_dict(checkpoint[\"target_dict\"])\n",
    "        self.optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "        self.sars = checkpoint[\"replay\"]\n",
    "        self.eps = checkpoint[\"epsilon\"]\n",
    "        self.reward_history = checkpoint[\"reward_history\"]\n",
    "        self.loss_history = checkpoint[\"loss_history\"]\n",
    "        self.num_train_episodes = checkpoint[\"num_train_episodes\"]\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, terminal):\n",
    "        # print(\"store_transition\")\n",
    "        l = list(np.array(state[0]).flatten().astype(int)) + list(\n",
    "            np.array(state[1]).flatten().astype(int)\n",
    "        )\n",
    "        # print(\"store\", state)\n",
    "        l.append(int(state[2][0]))\n",
    "        l.append(int(state[2][1]))\n",
    "        l.append(int(state[3][0]))\n",
    "        l.append(int(state[3][1]))\n",
    "        new_state = np.array(l)\n",
    "        l = list(np.array(state_[0]).flatten().astype(int)) + list(\n",
    "            np.array(state_[1]).flatten().astype(int)\n",
    "        )\n",
    "        l.append(int(state_[2][0]))\n",
    "        l.append(int(state_[2][1]))\n",
    "        l.append(int(state_[3][0]))\n",
    "        l.append(int(state_[3][1]))\n",
    "        new_state_ = np.array(l)\n",
    "        # print(\"store\", action)\n",
    "        if len(action) > 1:\n",
    "            new_action = self.action_list.index(action)\n",
    "        data = np.zeros((1, 2 * len(new_state) + 1 + 1))\n",
    "        data[0, 0 : len(new_state)] = new_state\n",
    "\n",
    "        data[0, len(new_state)] = new_action\n",
    "        reward_idx = len(new_state) + 1\n",
    "        data[0, reward_idx] = reward\n",
    "        next_features_idx = len(new_state) + 1 + 1\n",
    "        data[0, next_features_idx:] = new_state_\n",
    "\n",
    "        if self.sars is None:\n",
    "            self.sars = data\n",
    "        else:\n",
    "            self.sars = np.vstack((self.sars, data))\n",
    "        # drop from memory if too many elements\n",
    "        if self.sars.shape[0] > self.mem_size:\n",
    "            if self.sars.shape[0] - self.mem_size > self.batch_size:\n",
    "                sars = []\n",
    "                for i in range(int((self.sars.shape[0] - self.mem_size) / 2)):\n",
    "                    sars.append(\n",
    "                        self.sars[int((self.sars.shape[0] - self.mem_size) / 2) + i - 1]\n",
    "                    )\n",
    "        \"\"\"\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        l = list(np.array(state[0]).flatten().astype(int)) + list(np.array(state[1]).flatten().astype(int))\n",
    "        l.append(int(state[2][0]))\n",
    "        l.append(int(state[2][1]))\n",
    "        l.append(int(state[3][0]))\n",
    "        l.append(int(state[3][1]))\n",
    "        self.state_memory[index] = T.tensor(l).type(T.LongTensor)\n",
    "        l = list(np.array(state_[0]).flatten().astype(int)) + list(np.array(state_[1]).flatten().astype(int))\n",
    "        l.append(int(state_[2][0]))\n",
    "        l.append(int(state_[2][1]))\n",
    "        l.append(int(state_[3][0]))\n",
    "        l.append(int(state_[3][1]))\n",
    "        self.new_state_memory[index] = T.tensor(l).type(T.LongTensor)\n",
    "        self.reward_memory[index] = reward\n",
    "        #print(self.action_memory.shape)\n",
    "        #print(action)\n",
    "        indx = self.action_list.index(action)\n",
    "        array = np.zeros(self.n_actions)\n",
    "        array[indx] = 1\n",
    "        self.action_memory[index] =  T.tensor(array).type(T.LongTensor)\n",
    "        self.terminal_memory[index] = terminal\n",
    "        self.mem_cntr += 1\"\"\"\n",
    "        if self.best_SARS[2] < reward:\n",
    "            self.best_SARS = [state, action, reward, state_, terminal]\n",
    "\n",
    "    def update_Q_network(self):\n",
    "        # print(\"updating Q net for agent\"+str(self.id))\n",
    "        if (\n",
    "            self.sars is None\n",
    "            or self.sars.shape[0] < self.min_exp\n",
    "            or self.sars.shape[0] < self.batch_size\n",
    "        ):\n",
    "            return\n",
    "        # print(\"in update\")\n",
    "        # create mini-batch\n",
    "        # print(\"sars: \",self.sars)\n",
    "        batch = self.sars[\n",
    "            np.random.choice(self.sars.shape[0], self.batch_size, replace=False), :\n",
    "        ]\n",
    "        batch_features = torch.from_numpy(batch[:, 0 : self.input_dims])\n",
    "        batch_features = Variable(batch_features.int())\n",
    "        batch_actions = torch.from_numpy(batch[:, self.input_dims]).type(torch.int64)\n",
    "        batch_actions = Variable(batch_actions)\n",
    "        x = (\n",
    "            self.model(batch_features.float())\n",
    "            .gather(1, batch_actions.view(-1, 1))\n",
    "            .squeeze()\n",
    "        )\n",
    "\n",
    "        # calculate loss\n",
    "        batch_rewards = batch[:, self.input_dims + 1]\n",
    "        next_features_idx = self.input_dims + 1 + 1\n",
    "        batch_next_features = Variable(torch.from_numpy(batch[:, next_features_idx:]))\n",
    "        tt = self.target(batch_next_features.float()).data.cpu().numpy()\n",
    "        tt = batch_rewards + self.gamma * np.amax(tt, axis=1)\n",
    "        tt = Variable(torch.from_numpy(tt).float(), requires_grad=False)\n",
    "        loss = self.loss_fn(x, tt)\n",
    "\n",
    "        self.loss_history.append(loss.item())\n",
    "\n",
    "        # back propagate\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.epsilon = (\n",
    "            self.epsilon - self.eps_dec if self.epsilon > self.eps_min else self.eps_min\n",
    "        )\n",
    "        if self.model_updates % self.replace_target == 0:\n",
    "            self.target = copy.deepcopy(self.model)\n",
    "        self.model_updates += 1\n",
    "\n",
    "        \"\"\"\n",
    "        if self.mem_cntr < self.batch_size:\n",
    "            return\n",
    "        #print(\"after if\")\n",
    "\n",
    "        self.Q_eval.optimizer.zero_grad()\n",
    "        \n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "\n",
    "        batch = np.random.choice(max_mem, self.batch_size, replace=False)\n",
    "        \n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "\n",
    "        state_batch = T.tensor(self.state_memory[batch]).to(self.Q_eval.device)\n",
    "        new_state_batch = T.tensor(self.new_state_memory[batch]).to(self.Q_eval.device)\n",
    "        action_batch = self.action_memory[batch]\n",
    "        reward_batch = T.tensor(self.reward_memory[batch]).to(self.Q_eval.device)\n",
    "        terminal_batch = T.tensor(self.terminal_memory[batch]).to(self.Q_eval.device)\n",
    "        #????\n",
    "        #print(self.Q_eval(state_batch),self.Q_eval(state_batch).shape)\n",
    "        #l = self.action_list.index(action)\n",
    "        #print(self.Q_eval(state_batch))\n",
    "        q_eval = self.Q_eval(state_batch)[batch_index,action_batch]\n",
    "        q_next = self.Q_eval(new_state_batch)\n",
    "        q_next[terminal_batch] = 0.0\n",
    "\n",
    "        q_target = reward_batch + self.gamma*T.max(q_next,dim=1)[0]\n",
    "\n",
    "        loss = self.Q_eval.loss(q_target, q_eval).to(self.Q_eval.device)\n",
    "        loss.backward()\n",
    "        self.Q_eval.optimizer.step()\n",
    "        #print(\"out of function\")\n",
    "\n",
    "        self.iter_cntr += 1\n",
    "        self.epsilon = self.epsilon - self.eps_dec if self.epsilon > self.eps_min \\\n",
    "                       else self.eps_min\"\"\"\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        # print(\"choosing action for agent:\"+str(self.id))\n",
    "        if True:  # np.array(observation).ndim != 1:\n",
    "            l = list(np.array(observation[0]).flatten().astype(int)) + list(\n",
    "                np.array(observation[1]).flatten().astype(int)\n",
    "            )\n",
    "            l.append(float(observation[2][0]))\n",
    "            l.append(float(observation[2][1]))\n",
    "            l.append(float(observation[3][0]))\n",
    "            l.append(float(observation[3][1]))\n",
    "            observation = np.array(l)\n",
    "        if np.random.random() > self.epsilon:\n",
    "            q_features = Variable(torch.from_numpy(observation)).type(T.long)\n",
    "            q_values = (\n",
    "                self.model(torch.from_numpy(observation).float())[0].data.cpu().numpy()\n",
    "            )\n",
    "            action = self.action_list[np.argmax(q_values)]\n",
    "        else:\n",
    "            if np.random.random() > self.epsilon / 3:\n",
    "                action = self.heuristic()\n",
    "            else:\n",
    "                action = self.action_list[\n",
    "                    np.random.choice(self.action_space)\n",
    "                ]  # self.action_space\n",
    "        return action\n",
    "\n",
    "    def heuristic(self):\n",
    "        action = [0, 0, 0]\n",
    "        # if agent reached fire, move on the fire boundary to apply retardant\n",
    "        if self.observ1[1][1] == 1:\n",
    "            action[0] = 1\n",
    "        if True:\n",
    "            # find the action that is closest to simply rotating clockwise about the fire center\n",
    "            if self.position[0] < self.mean_pos_fire[0]:\n",
    "                if self.position[1] < self.mean_pos_fire[1]:\n",
    "                    action[1] = 2\n",
    "                elif self.position[1] == self.mean_pos_fire[1]:\n",
    "                    action[1] = 1\n",
    "                else:\n",
    "                    action[1] = 8\n",
    "            elif self.position[0] == self.mean_pos_fire[0]:\n",
    "                if self.position[1] < self.mean_pos_fire[1]:\n",
    "                    action[1] = 3\n",
    "                elif self.position[1] == self.mean_pos_fire[1]:\n",
    "                    action[1] = 0\n",
    "                    action[0] = 1\n",
    "                else:\n",
    "                    action[1] = 7\n",
    "            else:\n",
    "                if self.position[1] < self.mean_pos_fire[1]:\n",
    "                    action[1] = 4\n",
    "                elif self.position[1] == self.mean_pos_fire[1]:\n",
    "                    action[1] = 5\n",
    "                else:\n",
    "                    action[1] = 6\n",
    "            fire_center_vector = [\n",
    "                self.position[0] - self.mean_pos_fire[0],\n",
    "                self.position[1] - self.mean_pos_fire[1],\n",
    "            ]\n",
    "        return action\n",
    "\n",
    "    # amir\n",
    "    def update_state(self, observ1, observ2):\n",
    "        self.observ[0] = observ1\n",
    "        self.observ[1] = observ2\n",
    "\n",
    "    # man\n",
    "    def update_position(self, action_array, m):\n",
    "        m = m - 1\n",
    "        self.past_position = (self.position[0], self.position[1])\n",
    "        position = self.position\n",
    "        move = [0, 0]\n",
    "        action = action_array\n",
    "        if action == 1:\n",
    "            move = [1, 0]\n",
    "        elif action == 2:\n",
    "            move = [1, 1]\n",
    "        elif action == 3:\n",
    "            move = [0, 1]\n",
    "        elif action == 4:\n",
    "            move = [-1, 1]\n",
    "        elif action == 5:\n",
    "            move = [-1, 0]\n",
    "        elif action == 6:\n",
    "            move = [-1, -1]\n",
    "        elif action == 7:\n",
    "            move = [0, -1]\n",
    "        elif action == 8:\n",
    "            move = [1, -1]\n",
    "        position_new = np.array(position) + move\n",
    "        if (\n",
    "            position_new[0] > m\n",
    "            or position_new[0] < 0\n",
    "            or position_new[1] > m\n",
    "            or position_new[1] < 0\n",
    "        ):\n",
    "            position = position\n",
    "        else:\n",
    "            position = (position_new[0], position_new[1])\n",
    "        self.position = (position[0], position[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 10\n",
    "indiviual_reward_importance = 0.1\n",
    "social_reward_importance = 0.1\n",
    "p_change_wind = 0.001\n",
    "type_plane = [[0, 1, 2], [0.005, 0.9, 0.095]]\n",
    "P_burn = 0.1\n",
    "P_set_fire = 0.1\n",
    "P_fire_depend_on_wind = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forest = Forest(\n",
    "#     size,\n",
    "#     indiviual_reward_importance,\n",
    "#     social_reward_importance,\n",
    "#     p_change_wind,\n",
    "#     type_plane,\n",
    "#     P_burn,\n",
    "#     P_set_fire,\n",
    "#     # P_fire_depend_on_wind,\n",
    "# )\n",
    "# # gen = Genetic(f)\n",
    "# # r, agents = main_func(gen.make_arch([2, 2, 2]), forest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! Testing agent\n",
    "\n",
    "# min_agent = mini_Agent(1,0,0,forest)\n",
    "# min_agent.choose_action(np.zeros((3,3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    # amir\n",
    "    def __init__(self, env, arch, read=None, record_flag=False):\n",
    "        # init mini_Agents =>[]\n",
    "        self.counter = 1\n",
    "        self.all_agents = []\n",
    "        self.env = env\n",
    "        self.arch = arch\n",
    "        self.record_flag = record_flag\n",
    "        if read is not None:\n",
    "            if read == 1:\n",
    "                for a in arch:\n",
    "                    self.all_agents.append(\n",
    "                        mini_Agent(a[0], a[2], a[1], env, a[0])\n",
    "                    )  # [mini_Agent() for agent in range(agent_number)]\n",
    "            else:\n",
    "                for a in arch:\n",
    "                    self.all_agents.append(mini_Agent(a[0], a[2], a[1], env, 0))\n",
    "        else:\n",
    "            for a in arch:\n",
    "                self.all_agents.append(\n",
    "                    mini_Agent(a[0], a[2], a[1], env)\n",
    "                )  # [mini_Agent() for agent in range(agent_number)]\n",
    "        # self.agents_info =\n",
    "        # for mini_Agent id,level,parent [id,parent_id,level]\n",
    "        #\n",
    "\n",
    "        return\n",
    "\n",
    "    def take_action(self, actions, positions):\n",
    "        # print(\"taking action\")\n",
    "        # print(actions)\n",
    "        j = 0\n",
    "        for agent in self.all_agents:\n",
    "            if actions[j][1] != 0:\n",
    "                agent.update_position(actions[j][1], len(self.env.grid_env))\n",
    "            j = j + 1\n",
    "        r, terminated, observations = self.env.step(actions, positions)\n",
    "        for j in range(len(self.all_agents)):\n",
    "            agent = self.all_agents[j]\n",
    "\n",
    "        # for each agent state agent update\n",
    "        # for each agent if action = move => update position\n",
    "        # for each agent if action == help => init new mini_Agent with Net Queen\n",
    "        # returns reward, terminated\n",
    "        self.counter += 1\n",
    "        return r, terminated, observations\n",
    "\n",
    "    def communicate(self, level):  # comunication: level down to up\n",
    "        # define agents with level\n",
    "        lev_1 = [x for x in level if x[2] == 0]\n",
    "        lev_2 = [x for x in level if x[2] == 1]\n",
    "        lev_3 = [x for x in level if x[2] == 2]\n",
    "        all_gents_level = [lev_1, lev_2, lev_3]\n",
    "        for same_level in all_gents_level:\n",
    "            for agent in same_level:\n",
    "                parent_i = agent[1]\n",
    "                id_i = agent[0]\n",
    "                agent = self.all_agents[id_i]\n",
    "                mean_x_fire = self.all_agents[id_i].mean_pos_fire[0]\n",
    "                mean_y_fire = self.all_agents[id_i].mean_pos_fire[1]\n",
    "                counter = 0\n",
    "                same_levels_for_comparison = [] + same_level\n",
    "                same_levels_for_comparison.remove(agent)\n",
    "                while counter < len(same_levels_for_comparison):\n",
    "                    new_parent = same_levels_for_comparison[counter][1]\n",
    "                    new_id = same_levels_for_comparison[counter][0]\n",
    "                    new_agent = self.all_agents[new_id]\n",
    "                    if parent_i == new_parent:\n",
    "                        agent.mean_pos_fire = (\n",
    "                            int(np.mean([mean_x_fire, new_agent.mean_pos_fire[0]])),\n",
    "                            int(np.mean([mean_y_fire, new_agent.mean_pos_fire[1]])),\n",
    "                        )\n",
    "                        # print(new_agent.best_SARS)\n",
    "                        if (\n",
    "                            np.sum(new_agent.best_SARS[0][0])\n",
    "                            + np.sum(new_agent.best_SARS[0][1])\n",
    "                            + np.sum(new_agent.best_SARS[1])\n",
    "                            + np.sum(new_agent.best_SARS[2])\n",
    "                            + np.sum(new_agent.best_SARS[3][0])\n",
    "                            + np.sum(new_agent.best_SARS[3][1])\n",
    "                            + np.sum(new_agent.best_SARS[4])\n",
    "                        ) != 0:\n",
    "                            agent.store_transition(\n",
    "                                new_agent.best_SARS[0],\n",
    "                                new_agent.best_SARS[1],\n",
    "                                new_agent.best_SARS[2],\n",
    "                                new_agent.best_SARS[3],\n",
    "                                new_agent.best_SARS[4],\n",
    "                            )\n",
    "                    counter = counter + 1\n",
    "        return\n",
    "\n",
    "    def live_one_episode(self):\n",
    "        # while not terminated:\n",
    "        #       for each agent:\n",
    "        #           choose action ( action = mini_agent.choose_action(observation) )\n",
    "        #           take_action  (reward, done, obsv = env.step(action))\n",
    "        #           mini_agent.store_transition(observation, action, reward, obsv, done)\n",
    "        #           communication\n",
    "        #           mini_agent.store_transition(based on communication)\n",
    "        #           mini_agent.learn()\n",
    "        #           observation = obsv\n",
    "        #           save(score,....?)\n",
    "        total_R = []\n",
    "        terminated = False\n",
    "        while not terminated:\n",
    "            actions = []\n",
    "            positions = []\n",
    "            for j in range(len(self.all_agents)):\n",
    "                agent = self.all_agents[j]\n",
    "                # print(agent.observ)\n",
    "                # print(agent)\n",
    "                actions.append(agent.choose_action(agent.observ))\n",
    "                positions.append(agent.position)\n",
    "            len_before = len(self.all_agents)\n",
    "            # print(\"len before:\"+str(len(self.all_agents)))\n",
    "            reward, terminated, obsv = self.take_action(actions, positions)\n",
    "            for j in range(len_before):\n",
    "                agent = self.all_agents[j]\n",
    "                print(agent.observ)\n",
    "                obsp = np.copy(agent.observ)\n",
    "                obsp[2] = agent.past_position\n",
    "                # print(j,len(obsv),len(self.all_agents), len(positions))\n",
    "                agent.observ[0] = obsv[j][0]\n",
    "                agent.observ[1] = obsv[j][1]\n",
    "                agent.observ[2] = agent.position\n",
    "                if any(np.array(agent.observ[0]).flatten()) == 1:\n",
    "                    # print(agent.mean_pos_fire[0],agent.position[0])\n",
    "                    agent.mean_pos_fire = (\n",
    "                        np.mean((agent.position[0], agent.mean_pos_fire[0])),\n",
    "                        np.mean((agent.position[1], agent.mean_pos_fire[1])),\n",
    "                    )\n",
    "                    agent.observ[3] = agent.mean_pos_fire\n",
    "                agent.store_transition(\n",
    "                    obsp, actions[j], reward[j], np.copy(agent.observ), terminated\n",
    "                )\n",
    "            self.communicate(self.arch)\n",
    "            for agent in self.all_agents:\n",
    "                agent.update_Q_network()\n",
    "            # print(\"1111\")\n",
    "            if self.record_flag:\n",
    "                self.env.render(positions)\n",
    "            total_R.append(reward)\n",
    "        if self.record_flag:\n",
    "            self.env.save_episode()\n",
    "        return total_R\n",
    "\n",
    "        # while not terminated\n",
    "        #       for each agent choose action\n",
    "        #       take_action\n",
    "        #       communication\n",
    "        #       for each agent update_Q_network based on reward and communication\n",
    "\n",
    "    # communication :(mean_x_fire,mean_y_fire,best_state_action,R,level)\n",
    "    # state=[3X3],[3X3],x,y,mean_x_fire,mean_y_fire\n",
    "    # Action: fire_r => 0,1 , move => 0,...,8, help => 0,1, =>[[0,1],[0,...,8],[0,1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from geneticalgorithm2 import (\n",
    "    geneticalgorithm2 as ga,\n",
    ")  # for creating and running optimization model\n",
    "\n",
    "from geneticalgorithm2 import (\n",
    "    Crossover,\n",
    "    Mutations,\n",
    "    Selection,\n",
    ")  # classes for specific mutation and crossover behavior\n",
    "\n",
    "from geneticalgorithm2 import (\n",
    "    Population_initializer,\n",
    ")  # for creating better start population\n",
    "\n",
    "from geneticalgorithm2 import np_lru_cache  # for cache function (if u want)\n",
    "\n",
    "from geneticalgorithm2 import (\n",
    "    plot_pop_scores,\n",
    ")  # for plotting population scores, if u want\n",
    "\n",
    "from geneticalgorithm2 import Callbacks  # simple callbacks\n",
    "\n",
    "from geneticalgorithm2 import (\n",
    "    Actions,\n",
    "    ActionConditions,\n",
    "    MiddleCallbacks,\n",
    ")  # middle callbacks\n",
    "\n",
    "\n",
    "class Genetic:\n",
    "    # man\n",
    "    def __init__(self, env, check_point=None):\n",
    "        self.env = env\n",
    "        self.k = 0\n",
    "        self.check_point = check_point\n",
    "        # [i,j,k,n]=> i+j+k = n i=#queen, j=#level1, k=#level2\n",
    "        # population random =>p0 => live in env for one episode => fitness => 100 population => best\n",
    "        # => make hirearchy => train => select_best\n",
    "        return\n",
    "\n",
    "    def init_start_generation(self, N, min_agents, max_agents):\n",
    "        start_generation = np.zeros((N, 3))\n",
    "        for i in range(N):\n",
    "            while True:\n",
    "                agent = np.random.choice(\n",
    "                    [i + min_agents for i in range(max_agents + 1 - min_agents)]\n",
    "                )\n",
    "                if agent != 0:\n",
    "                    break\n",
    "            Queen = np.random.choice([i for i in range(agent + 1)])\n",
    "            if Queen != 0 and Queen != agent:\n",
    "                while True:\n",
    "                    lv1 = np.random.choice([i for i in range(agent + 1 - Queen)])\n",
    "                    if lv1 != 0:\n",
    "                        break\n",
    "                start_generation[i] = np.array([Queen, lv1, agent - lv1 - Queen])\n",
    "            elif Queen == 0:\n",
    "                lv1 = np.random.choice([i for i in range(agent + 1)])\n",
    "                start_generation[i] = np.array([Queen, lv1, agent - lv1 - Queen])\n",
    "            else:\n",
    "                start_generation[i] = np.array([Queen, 0, 0])\n",
    "        return start_generation\n",
    "\n",
    "    def make_arch(self, X):\n",
    "        level = []\n",
    "        if X[0] == 0 and X[1] == 0:\n",
    "            for i in range(int(X[2])):\n",
    "                level.append([i, -1, 2])\n",
    "            return level\n",
    "        if X[0] == 0 and X[2] == 0:\n",
    "            for i in range(int(X[1])):\n",
    "                level.append([i, -1, 1])\n",
    "            return level\n",
    "        if X[1] == 0 and X[2] == 0:\n",
    "            for i in range(int(X[0])):\n",
    "                level.append([i, -1, 0])\n",
    "            return level\n",
    "        if X[0] == 0:\n",
    "            for i in range(int(X[1])):\n",
    "                level.append([i, -1, 1])\n",
    "        i = 0\n",
    "        queen = []\n",
    "        for q in range(int(X[0])):\n",
    "            level.append([i, -1, 0])\n",
    "            queen.append(i)\n",
    "            i = i + 1\n",
    "        lv1 = []\n",
    "        for q in range(int(X[1])):\n",
    "            lv1.append(i)\n",
    "            i = i + 1\n",
    "        lv2 = []\n",
    "        for q in range(int(X[2])):\n",
    "            lv2.append(i)\n",
    "            i = i + 1\n",
    "        temp = np.copy(lv1)\n",
    "        if len(queen) != 0:\n",
    "            while True:\n",
    "                for q in queen:\n",
    "                    if len(lv1) == 0:\n",
    "                        break\n",
    "                    c = np.random.choice(lv1)\n",
    "                    lv1.remove(c)\n",
    "                    level.append([c, q, 1])\n",
    "\n",
    "                if len(lv1) == 0:\n",
    "                    break\n",
    "        if len(temp) != 0:\n",
    "            while True:\n",
    "                for q in temp:\n",
    "                    if len(lv2) == 0:\n",
    "                        break\n",
    "                    c = np.random.choice(lv2)\n",
    "                    lv2.remove(c)\n",
    "                    level.append([c, q, 2])\n",
    "\n",
    "                if len(lv2) == 0:\n",
    "                    break\n",
    "        return level\n",
    "\n",
    "    def fitness_function(self, X):\n",
    "        # print(\"ep\"+str(self.k))\n",
    "        self.k = self.k + 1\n",
    "        arch = self.make_arch(X)\n",
    "        agents = Agent(self.env, arch, self.check_point)\n",
    "        Total = []\n",
    "        for i in range(2):\n",
    "            r = agents.live_one_episode()\n",
    "            self.env.reset()\n",
    "            sum = 0\n",
    "            for x in r:\n",
    "                sum = sum + np.sum(x)\n",
    "            Total.append(sum)\n",
    "        # print(\"Reward\")\n",
    "        sum = np.sum(Total)\n",
    "        # print(sum)\n",
    "        return -1 * sum\n",
    "\n",
    "    def my_mutation(self, current_value, left_border, right_border):\n",
    "        # print(\"muting\")\n",
    "        # print(current_value, left_border, right_border)\n",
    "        return\n",
    "\n",
    "    def my_crossover(self, parent_a, parent_b):\n",
    "        child_1 = [int(parent_a[0]), int(parent_a[1]), int(parent_b[2])]\n",
    "        if child_1[1] == 0:\n",
    "            if child_1[2] != 0 and child_1[0] != 0:\n",
    "                child_1[1] = 1\n",
    "            if child_1[2] == 0 and child_1[0] == 0:\n",
    "                child_1[1] = 1\n",
    "        child_2 = [int(parent_b[0]), int(parent_b[1]), int(parent_a[2])]\n",
    "        if child_1[1] == 0:\n",
    "            if child_1[2] != 0 and child_1[0] != 0:\n",
    "                child_1[1] = 1\n",
    "            if child_1[2] == 0 and child_1[0] == 0:\n",
    "                child_1[1] = 1\n",
    "        return np.array(child_1), np.array(child_2)\n",
    "\n",
    "    def train(self):\n",
    "        start_generation = self.init_start_generation(100, 0, 5)\n",
    "        varbound = np.array([[0, 5]] * 3)\n",
    "        model = ga(\n",
    "            function=self.fitness_function,\n",
    "            dimension=3,\n",
    "            variable_type=\"int\",\n",
    "            variable_boundaries=varbound,\n",
    "            variable_type_mixed=None,\n",
    "            function_timeout=10,\n",
    "            algorithm_parameters={\n",
    "                \"max_num_iteration\": None,\n",
    "                \"population_size\": 100,\n",
    "                \"mutation_probability\": 1,\n",
    "                \"elit_ratio\": 0.01,\n",
    "                \"crossover_probability\": 0.5,\n",
    "                \"parents_portion\": 0.3,\n",
    "                \"crossover_type\": self.my_crossover,\n",
    "                \"mutation_type\": self.my_mutation,\n",
    "                \"selection_type\": \"roulette\",\n",
    "                \"max_iteration_without_improv\": None,\n",
    "            },\n",
    "        )\n",
    "        model.run(\n",
    "            no_plot=False,\n",
    "            start_generation={\"variables\": start_generation, \"scores\": None},\n",
    "        )\n",
    "        convergence = model.report\n",
    "        return convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_func(arch, env, read=None, record=False):\n",
    "    agents = Agent(env, arch, read, record)\n",
    "    Total_r = []\n",
    "    for i in range(10):\n",
    "        print(\"-------------------------------------------------------\")\n",
    "        print(\"episode Of : \", i)\n",
    "        r = agents.live_one_episode()\n",
    "        # print(\"reward of \"+str(i)+\"th iteration is:\" + str(r))\n",
    "        env.reset()\n",
    "        Total_r.append(r)\n",
    "    for agent in agents.all_agents:\n",
    "        agent.save_checkpoint()\n",
    "    return Total_r, agents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 10\n",
    "indiviual_reward_importance = 0.1\n",
    "social_reward_importance = 0.1\n",
    "p_change_wind = 0.001\n",
    "type_plane = [[0, 1, 2], [0.005, 0.9, 0.095]]\n",
    "P_burn = 0.1\n",
    "P_set_fire = 0.1\n",
    "P_fire_depend_on_wind = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "episode Of :  0\n",
      "[array([[0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.]]), array([[0., 1., 0.],\n",
      "       [0., 1., 0.],\n",
      "       [0., 1., 0.]]), (7, 9), (0, 2)]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (4,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/adil/Desktop/Forest_Fire_Copied/another_test.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/adil/Desktop/Forest_Fire_Copied/another_test.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m forest \u001b[39m=\u001b[39m Forest(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/adil/Desktop/Forest_Fire_Copied/another_test.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     size,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/adil/Desktop/Forest_Fire_Copied/another_test.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     indiviual_reward_importance,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/adil/Desktop/Forest_Fire_Copied/another_test.ipynb#X20sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     P_fire_depend_on_wind,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/adil/Desktop/Forest_Fire_Copied/another_test.ipynb#X20sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/adil/Desktop/Forest_Fire_Copied/another_test.ipynb#X20sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m gen \u001b[39m=\u001b[39m Genetic(f)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/adil/Desktop/Forest_Fire_Copied/another_test.ipynb#X20sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m r, agents \u001b[39m=\u001b[39m main_func(gen\u001b[39m.\u001b[39;49mmake_arch([\u001b[39m2\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m2\u001b[39;49m]), forest)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/adil/Desktop/Forest_Fire_Copied/another_test.ipynb#X20sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# for a in agents.all_agents:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/adil/Desktop/Forest_Fire_Copied/another_test.ipynb#X20sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m#     a.save_checkpoint()\u001b[39;00m\n",
      "\u001b[1;32m/home/adil/Desktop/Forest_Fire_Copied/another_test.ipynb Cell 15\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/adil/Desktop/Forest_Fire_Copied/another_test.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m-------------------------------------------------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/adil/Desktop/Forest_Fire_Copied/another_test.ipynb#X20sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mepisode Of : \u001b[39m\u001b[39m\"\u001b[39m, i)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/adil/Desktop/Forest_Fire_Copied/another_test.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m r \u001b[39m=\u001b[39m agents\u001b[39m.\u001b[39;49mlive_one_episode()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/adil/Desktop/Forest_Fire_Copied/another_test.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# print(\"reward of \"+str(i)+\"th iteration is:\" + str(r))\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/adil/Desktop/Forest_Fire_Copied/another_test.ipynb#X20sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m env\u001b[39m.\u001b[39mreset()\n",
      "\u001b[1;32m/home/adil/Desktop/Forest_Fire_Copied/another_test.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/adil/Desktop/Forest_Fire_Copied/another_test.ipynb#X20sZmlsZQ%3D%3D?line=119'>120</a>\u001b[0m agent \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_agents[j]\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/adil/Desktop/Forest_Fire_Copied/another_test.ipynb#X20sZmlsZQ%3D%3D?line=120'>121</a>\u001b[0m \u001b[39mprint\u001b[39m(agent\u001b[39m.\u001b[39mobserv)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/adil/Desktop/Forest_Fire_Copied/another_test.ipynb#X20sZmlsZQ%3D%3D?line=121'>122</a>\u001b[0m obsp \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mcopy(agent\u001b[39m.\u001b[39;49mobserv)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/adil/Desktop/Forest_Fire_Copied/another_test.ipynb#X20sZmlsZQ%3D%3D?line=122'>123</a>\u001b[0m obsp[\u001b[39m2\u001b[39m] \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mpast_position\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/adil/Desktop/Forest_Fire_Copied/another_test.ipynb#X20sZmlsZQ%3D%3D?line=123'>124</a>\u001b[0m \u001b[39m# print(j,len(obsv),len(self.all_agents), len(positions))\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mcopy\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/lib/function_base.py:960\u001b[0m, in \u001b[0;36mcopy\u001b[0;34m(a, order, subok)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_copy_dispatcher)\n\u001b[1;32m    872\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcopy\u001b[39m(a, order\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mK\u001b[39m\u001b[39m'\u001b[39m, subok\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    873\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    874\u001b[0m \u001b[39m    Return an array copy of the given object.\u001b[39;00m\n\u001b[1;32m    875\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    958\u001b[0m \n\u001b[1;32m    959\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 960\u001b[0m     \u001b[39mreturn\u001b[39;00m array(a, order\u001b[39m=\u001b[39;49morder, subok\u001b[39m=\u001b[39;49msubok, copy\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (4,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "forest = Forest(\n",
    "    size,\n",
    "    indiviual_reward_importance,\n",
    "    social_reward_importance,\n",
    "    p_change_wind,\n",
    "    type_plane,\n",
    "    P_burn,\n",
    "    P_set_fire,\n",
    "    P_fire_depend_on_wind,\n",
    ")\n",
    "gen = Genetic(f)\n",
    "r, agents = main_func(gen.make_arch([2, 2, 2]), forest)\n",
    "# for a in agents.all_agents:\n",
    "#     a.save_checkpoint()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
